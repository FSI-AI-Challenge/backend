from langgraph.graph import StateGraph, add_messages, END
from langgraph.types import interrupt, Command
from typing import Annotated, Dict, List, Optional, Tuple, TypedDict
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timezone, timedelta
from langchain_ollama import ChatOllama
import json
import traceback

llm = ChatOllama(model='gpt-oss:20b', streaming=True)

@dataclass
class Goal:
    target_amount: int
    target_months: int         

@dataclass
class IncomeExpense:
    fixed_income: int                  # ì›”ê¸‰(ì„¸í›„ ë“± ê¸°ì¤€ í†µì¼)
    fixed_expense: int                   # ê³ ì •ì§€ì¶œ(ì›”)

class GraphState(TypedDict): 
    user_id: int
    created_ts: str
    question:str
    answer:str

    # ì…ë ¥/í”„ë¡œí•„
    goal: Goal
    investable_amount: int

    # ëŒ€í™” ë©”ì‹œì§€ (LangChain messages)
    messages: Annotated[List, add_messages]

def start(state:GraphState) -> GraphState:
    return GraphState()

def chatbot(state:GraphState) -> GraphState:
    print("ì±—ë´‡ ì‹œì‘")
    question = state['question']
    response = llm.invoke(question)
    print(f"ì±—ë´‡ ì¢…ë£Œ: {response.content}")
    return GraphState(answer=response.content)

def get_goal(state:GraphState) -> GraphState:
    print("ëª©í‘œ ê¸ˆì•¡, ê¸°ê°„ ì¶”ì¶œ ì‹œì‘")
    question = state['question']

    prompt = f"""
        Extract the user's savings goal.
        Return ONLY valid JSON, no markdown, no code block.
        Keys:
        - target_amount (int)
        - target_months (int)
        User input: {question}
    """

    response = llm.invoke(prompt)
    data = json.loads(response.content)
    
    print(f"ëª©í‘œ ê¸ˆì•¡, ê¸°ê°„ ì¶”ì¶œ ì¢…ë£Œ: {data}")
    return GraphState(
        goal=Goal(
            target_amount=int(data["target_amount"]), 
            target_months=int(data["target_months"])
        )
    )

def load_profile(state:GraphState) -> GraphState:
    print("ì‚¬ìš©ì ìˆ˜ì… ë° ì§€ì¶œ ê³„ì‚° ì‹œì‘")
    # ì¶”í›„ apië¡œ ëŒ€ì²´
    user_id = state['user_id']
    with open(f"./data/user_example_data{user_id}.json", "r", encoding="utf-8") as f:
        user_profile = json.load(f)

    ts = datetime.fromisoformat(state["created_ts"])
    current_year = ts.year
    current_month = ts.month

    recent_months = []
    for i in range(3):
        month = (current_month - i - 1) % 12 + 1
        year = current_year if current_month - i > 0 else current_year - 1
        recent_months.append(f"{year}-{month:02d}")

    recent_months = set(recent_months)

    filtered_months = [
        m for m in user_profile["months"] if m["month"] in recent_months
    ]

    template = '''
        You are an assistant that analyzes personal finance transaction data.

        Tasks:
        1) Identify the average fixed income per month.
        2) Identify the average fixed expenses per month.
        3) Identify the average variable expenses per month.
        4) Compute the average investable amount per month = fixed_income - (fixed_expenses + variable_expenses).

        Return ONLY valid JSON, no markdown, no code block. 
        Keys:
        - fixed_income (int)
        - fixed_expenses (int)
        - variable_expenses (int)
        - investable_amount (int)

        User Input:
        {}
    '''
    prompt = template.format(filtered_months)

    response = llm.invoke(prompt)
    print(response)
    data = json.loads(response.content)

    print(f"ì‚¬ìš©ì ìˆ˜ì… ë° ì§€ì¶œ ê³„ì‚° ì¢…ë£Œ: {data}")
    return GraphState(
        investable_amount=int(data["investable_amount"])
    )

def hitl_confirm_input(state:GraphState) -> GraphState:
    print("ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì‹œì‘")
    decision = interrupt({
        "step": "confirm_input",
        "message": "ëª©í‘œ ê¸ˆì•¡/ê¸°ê°„, íˆ¬ì ê°€ëŠ¥ ê¸ˆì•¡ì„ í™•ì¸ ë° ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
        "proposed": {
            "target_amount": state["goal"].target_amount,
            "target_months": state["goal"].target_months,
            "investable_amount": state["investable_amount"],
        },
        "fields": [
            {"name": "target_amount", "type": "number", "label": "ëª©í‘œ ê¸ˆì•¡(ì›)"},
            {"name": "target_months", "type": "number", "label": "ëª©í‘œ ê¸°ê°„(ê°œì›”)"},
            {"name": "investable_amount", "type": "number", "label": "íˆ¬ì ê°€ëŠ¥ ê¸ˆì•¡(ì›)"},
        ],
        "buttons": ["submit"]
    })
    
    target_amount = int(decision.get("target_amount", state["goal"].target_amount))
    target_months = int(decision.get("target_months", state["goal"].target_months))
    investable_amount = int(decision.get("investable_amount", state["investable_amount"]))

    if target_amount < 0 or target_months < 0 or investable_amount < 0:
        raise ValueError("ì…ë ¥ ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print(f"ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì¢…ë£Œ: {target_amount, target_months, investable_amount}")
    return GraphState(
        goal=Goal(
            target_amount=int(target_amount),
            target_months=int(target_months),
        ),
        investable_amount=int(investable_amount)
    )

def is_our_service(state:GraphState) -> str:
    print("ì„œë¹„ìŠ¤ íŒë‹¨ ì‹œì‘")
    question = state['question']

    prompt = f'''
        You are a classifier. Decide whether the user's message expresses intent to use a savings or investment guidance service.
        Answer only yes or no.

        User Input: {question}
    '''
    response = llm.invoke(prompt)
    print(f"ì„œë¹„ìŠ¤ íŒë‹¨ ì¢…ë£Œ: {response.content}")
    return response.content

graph = StateGraph(GraphState)

graph.add_node("start", start)
graph.add_node("chatbot", chatbot)
graph.add_node("get_goal", get_goal)
graph.add_node("load_profile", load_profile)
graph.add_node("hitl_confirm_input", hitl_confirm_input)

graph.set_entry_point("start")
graph.add_edge("start", "hitl_confirm_input")
graph.add_edge("hitl_confirm_input", END)
# graph.add_conditional_edges(
#     "start",
#     is_our_service,
#     {
#         "yes":"get_goal",
#         "no":"chatbot"
#     }
# )
# graph.add_edge("get_goal", "load_profile")
# graph.add_edge("load_profile", 'hitl_confirm_input')
# graph.add_edge("hitl_confirm_input", END)
# graph.add_edge("chatbot", END)

from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
agent = graph.compile(checkpointer=memory)

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import invoke_graph, stream_graph, random_uuid

KST = timezone(timedelta(hours=9))

user_id = 1
target_amount = 1000000
target_months = 6
created_ts = datetime.now(KST).isoformat()

config = RunnableConfig(recursion_limit=10, configurable={"thread_id":random_uuid()})

from typing import List, Literal, AsyncGenerator, Any
from pydantic import BaseModel
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from starlette.responses import StreamingResponse
from langgraph.types import Command
import asyncio
import os

# === (A) ê°œë°œ ì‹œ CORS í—ˆìš© ===
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # Vite dev ì„œë²„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

PROGRESS_MAP = {
    "is_our_service": ("is_our_service",  "ì¿¼ë¦¬ ë¶„ì„ ì¤‘..."),
    "chatbot": ("chatbot", "ì±—ë´‡ ë‹µë³€ ìƒì„± ì¤‘..."),
    "get_goal": ("get_goal", "ì‚¬ìš©ì ëª©í‘œ ê¸ˆì•¡ ë° ê¸°ê°„ ë¶„ì„ ì¤‘..."),
    "load_profile": ("load_profile", "ì‚¬ìš©ì ë°ì´í„° ê¸°ë°˜ íˆ¬ì ê°€ëŠ¥ ê¸ˆì•¡ ë¶„ì„ ì¤‘..."),
    "hitl_confirm_input": ("hitl_confirm_input", "ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì¤‘..."),
}

@app.post("/api/chat/stream")
async def chat_stream(req: Request):
    body = await req.json()
    
    if 'resume' in body:
        events = agent.astream_events(Command(resume=body.get('resume')), config=config, version="v1")
    else:
        question = next((m.get("content", "") for m in reversed(body.get("messages", [])) if m.get("role") == "user"), "")
        state_in = {
            "user_id": user_id,          
            "created_ts": created_ts,
            "question": question,
            "goal":Goal(
                target_amount=10000000,
                target_months=6,
            ),
            "investable_amount":678000
        }
        events = agent.astream_events(state_in, config=config, version="v1")

    async def eventgen():
        yield 'data: {"delta":" "}\n\n'  # í•¸ë“œì…°ì´í¬ (í”„ë¡ íŠ¸ onDelta íŠ¸ë¦¬ê±°)
        try:
            async for ev in events: 
                # print(ev) # evë¡œ ë§ˆì§€ë§‰ ë…¸ë“œëª… í™•ì¸í•˜ê³  ë…¸ë“œëª…ë„ ë³€ê²½í•´ì„œ ë§ˆì§€ë§‰ ê°’ë§Œ ì¶œë ¥
                etype = ev.get("event")
                node = ev.get("name")

                # ğŸ”¹ ì§„í–‰ ìƒíƒœ ì „ì†¡
                if etype == "on_chain_start":
                    step = PROGRESS_MAP.get(node)
                    if step:
                        sid, label = step
                        yield f'data: {json.dumps({"kind":"progress","id":sid,"status":"running","label":label}, ensure_ascii=False)}\n\n'

                if etype == "on_chain_end":
                    step = PROGRESS_MAP.get(node)
                    if step:
                        sid, _ = step
                        yield f'data: {json.dumps({"kind":"progress","id":sid,"status":"done"}, ensure_ascii=False)}\n\n'

                    data = ev.get("data")
                    output = data.get("output")

                    if output:
                        if '__interrupt__' in output:
                            raw = output['__interrupt__']
                            intr_obj = raw[0] if isinstance(raw, (list, tuple)) else raw
                            intr_payload = {
                                "value": getattr(intr_obj, "value", intr_obj),
                                "id": getattr(intr_obj, "id", None),
                            }
                            yield f'data: {json.dumps({"kind":"interrupt","payload": intr_payload}, ensure_ascii=False)}\n\n'
                            yield 'data: {"kind":"done"}\n\n'
                            return

                        # ì¼ë°˜ ë‹µë³€(ì˜ˆ: chatbot ë‹¨ê³„)ì„ deltaë¡œ í˜ë ¤ë³´ë‚´ê¸°
                        out_text = ""
                        if 'chatbot' in output:
                            out_text = output.get("chatbot", {}).get("answer", "")
                        # í•„ìš”ì‹œ ë‹¤ë¥¸ ë…¸ë“œ ì¶œë ¥ë„ í•©ì¹˜ê¸°
                        if out_text:
                            yield 'data: {"kind":"done"}\n\n'
                            for ch in out_text:
                                yield f'data: {json.dumps({"delta": ch}, ensure_ascii=False)}\n\n'
                                await asyncio.sleep(0.02)
                            return
        except Exception as e:
            yield f'data: {json.dumps({"delta": f"[server error] {type(e).__name__}: {e}"}, ensure_ascii=False)}\n\n'
            yield 'data: {"kind":"done"}\n\n'
            return

    return StreamingResponse(
        eventgen(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    )